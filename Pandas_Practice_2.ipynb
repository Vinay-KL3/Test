{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c52a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f54c80ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vinay_kl/Downloads/NLU2023-07-27 09:27:40.398515_Output.csv\n"
     ]
    }
   ],
   "source": [
    "v = datetime.datetime.now()\n",
    "\n",
    "#coll_whatsapp_stage\n",
    "EndPoint = \"https://stage-maia-whatsapp-nlu.saarthi.ai/predict\"\n",
    "\n",
    "dataCSV = \"/Users/vinay_kl/Downloads/testSheet.csv\"\n",
    "\n",
    "output = \"/Users/vinay_kl/Downloads/NLU\"+str(v)+\"_Output.csv\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec4f915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_pred_list = []\n",
    "intent_acc_list = []\n",
    "sub_intent_pred_list = []\n",
    "sub_intent_acc_list = []\n",
    "context_pred_list = []\n",
    "context_acc_list = []\n",
    "sub_context_pred_list = []\n",
    "sub_context_acc_list = []\n",
    "signal_pred_list = []\n",
    "signal_acc_list = []\n",
    "reason_pred_list = []\n",
    "reason_acc_list = []\n",
    "third_person_pred_list = []\n",
    "third_person_acc_list = []\n",
    "humiliate_pred_list = []\n",
    "humiliate_acc_list = []\n",
    "sentiment_pred_list = []\n",
    "sentiment_acc_list = []\n",
    "language_pred_list = []\n",
    "language_acc_list = []\n",
    "\n",
    "\n",
    "def NLU(text = None):\n",
    "    headers = {'Content-Type':'application/json'}    \n",
    "    payload = json.dumps ({\n",
    "        \n",
    "    \"text\": text\n",
    "                          })\n",
    "    response = requests.request(\"POST\", EndPoint, headers=headers, data=payload).json()\n",
    "\n",
    "\n",
    "\n",
    "    intent_pred = (response[\"response\"][\"intent\"][0][\"name\"])\n",
    "    intent_pred_list.append(intent_pred)\n",
    "    intent_acc = (response[\"response\"][\"intent\"][0][\"confidence\"])\n",
    "    intent_acc_list.append(intent_acc)\n",
    "\n",
    "    sub_intent_pred = (response[\"response\"][\"sub-intent\"][0][\"name\"])\n",
    "    sub_intent_pred_list.append(sub_intent_pred)\n",
    "    sub_intent_acc = (response[\"response\"][\"sub-intent\"][0][\"confidence\"])\n",
    "    sub_intent_acc_list.append(sub_intent_acc)\n",
    "\n",
    "    context_pred = (response[\"response\"][\"context\"][0][\"name\"])\n",
    "    context_pred_list.append(context_pred)\n",
    "    context_acc = (response[\"response\"][\"context\"][0][\"confidence\"])\n",
    "    context_acc_list.append(context_acc)\n",
    "\n",
    "    sub_context_pred = (response[\"response\"][\"sub-context\"][0][\"name\"])\n",
    "    sub_context_pred_list.append(sub_context_pred)\n",
    "    sub_context_acc = (response[\"response\"][\"sub-context\"][0][\"confidence\"])\n",
    "    sub_context_acc_list.append(sub_context_acc)\n",
    "\n",
    "    signal_pred = (response[\"response\"][\"signal\"][0][\"name\"])\n",
    "    signal_pred_list.append(signal_pred)\n",
    "    signal_acc = (response[\"response\"][\"signal\"][0][\"confidence\"])\n",
    "    signal_acc_list.append(signal_acc)\n",
    "\n",
    "    reason_pred = (response[\"response\"][\"delay_reason\"][0][\"name\"])\n",
    "    reason_pred_list.append(reason_pred)\n",
    "    reason_acc = (response[\"response\"][\"delay_reason\"][0][\"confidence\"])\n",
    "    reason_acc_list.append(reason_acc)\n",
    "\n",
    "    third_person_pred = (response[\"response\"][\"third_person\"][0][\"name\"])\n",
    "    third_person_pred_list.append(third_person_pred)\n",
    "    third_person_acc = (response[\"response\"][\"third_person\"][0][\"confidence\"])\n",
    "    third_person_acc_list.append(third_person_acc)\n",
    "\n",
    "    humiliate_pred = (response[\"response\"][\"humiliate\"][0][\"name\"])\n",
    "    humiliate_pred_list.append(humiliate_pred)\n",
    "    humiliate_acc = (response[\"response\"][\"humiliate\"][0][\"confidence\"])\n",
    "    humiliate_acc_list.append(humiliate_acc)\n",
    "\n",
    "    sentiment_pred = (response[\"response\"][\"sentiment\"][0][\"name\"])\n",
    "    sentiment_pred_list.append(sentiment_pred)\n",
    "    sentiment_acc = (response[\"response\"][\"sentiment\"][0][\"confidence\"])\n",
    "    sentiment_acc_list.append(sentiment_acc)\n",
    "\n",
    "    language_pred = (response[\"response\"][\"lang\"][0][\"name\"])\n",
    "    language_pred_list.append(language_pred)\n",
    "    language_acc = (response[\"response\"][\"lang\"][0][\"confidence\"])\n",
    "    language_acc_list.append(language_acc)\n",
    "    \n",
    "    \n",
    "    \n",
    "df = pd.read_csv(dataCSV)\n",
    "df.head()\n",
    "for TEXT in df[\"text\"]:\n",
    "    NLU(text = TEXT)\n",
    "    \n",
    "    \n",
    "    \n",
    "data = {'intent_pred': intent_pred_list,\n",
    "       'intent_acc': intent_acc_list,\n",
    "       'sub_intent_pred': sub_intent_pred_list,\n",
    "       'sub_intent_acc': sub_intent_acc_list,\n",
    "       'context_pred': context_pred_list,\n",
    "       'context_acc': context_acc_list,\n",
    "       'sub_context_pred': sub_context_pred_list,\n",
    "       'sub_context_acc': sub_context_acc_list,\n",
    "       'signal_pred': signal_pred_list,\n",
    "       'signal_acc': signal_acc_list,\n",
    "       'delay_reason_pred': reason_pred_list,\n",
    "       'delay_reason_acc': reason_acc_list,\n",
    "       'third_person_pred': third_person_pred_list,\n",
    "       'third_person_acc': third_person_acc_list,\n",
    "       'humiliate_pred': humiliate_pred_list,\n",
    "       'humiliate_acc': humiliate_acc_list,\n",
    "       'sentiment_pred': sentiment_pred_list,\n",
    "       'sentiment_acc': sentiment_acc_list,\n",
    "       'lang_pred': language_pred_list,\n",
    "       'lang_acc': language_acc_list}\n",
    "\n",
    "for column_name, values in data.items():\n",
    "    df[column_name] = values\n",
    "df.head(5)  \n",
    "\n",
    "df.to_csv(output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c3c534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_pred_comp = []\n",
    "\n",
    "\n",
    "for index, row in df.iterrows():  \n",
    "\n",
    "    if row[\"intent\"] == row[\"intent_pred\"]:\n",
    "        intent_pred_comp.append(\"correct_prediction\")\n",
    "    else:\n",
    "        intent_pred_comp.append(\"wrong_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "04025c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'correct_prediction', 'wrong_prediction', 'correct_prediction', 'correct_prediction']\n"
     ]
    }
   ],
   "source": [
    "print(intent_pred_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2d4e52a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row[i] \u001b[38;5;241m==\u001b[39m row[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)]:\n\u001b[0;32m---> 10\u001b[0m         (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pred_comp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m         (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pred_comp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrong_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "intent_pred_comp = []\n",
    "context_pred_comp = []\n",
    "\n",
    "\n",
    "L = [\"intent\", \"context\"]\n",
    "for i in L:\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        if row[i] == row[i+str(\"_pred\")]:\n",
    "            (i+str(\"_pred_comp\")).append(\"correct_prediction\")\n",
    "        else:\n",
    "            (i+str(\"_pred_comp\")).append(\"wrong_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aaaa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store the lists\n",
    "results = {\"intent\": [], \"context\": []}\n",
    "\n",
    "for column in [\"intent\", \"context\"]:\n",
    "    for index, row in df.iterrows():\n",
    "        if row[column] == row[column + \"_pred\"]:\n",
    "            results[column].append(\"correct_prediction\")\n",
    "        else:\n",
    "            results[column].append(\"wrong_prediction\")\n",
    "\n",
    "# Create new columns in the DataFrame and assign the lists from the dictionary\n",
    "df[\"intent_pred_comp\"] = results[\"intent\"]\n",
    "df[\"context_pred_comp\"] = results[\"context\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374e89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e76c1160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent_pred</th>\n",
       "      <th>intent_acc</th>\n",
       "      <th>sub_intent_pred</th>\n",
       "      <th>sub_intent_acc</th>\n",
       "      <th>context_pred</th>\n",
       "      <th>context_acc</th>\n",
       "      <th>sub_context_pred</th>\n",
       "      <th>sub_context_acc</th>\n",
       "      <th>signal_pred</th>\n",
       "      <th>signal_acc</th>\n",
       "      <th>reason_pred</th>\n",
       "      <th>reason_acc</th>\n",
       "      <th>third_person_pred</th>\n",
       "      <th>third_person_acc</th>\n",
       "      <th>humiliate_pred</th>\n",
       "      <th>humiliate_acc</th>\n",
       "      <th>sentiment_pred</th>\n",
       "      <th>sentiment_acc</th>\n",
       "      <th>language_pred</th>\n",
       "      <th>language_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inform</td>\n",
       "      <td>0.999714</td>\n",
       "      <td>general_ask_inform</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>bank_account</td>\n",
       "      <td>0.903529</td>\n",
       "      <td>general</td>\n",
       "      <td>0.999810</td>\n",
       "      <td>no_signal</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>account_closed</td>\n",
       "      <td>0.958353</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no_humiliation</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>hindi</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inform</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>general_ask_inform</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>bank_account</td>\n",
       "      <td>0.607388</td>\n",
       "      <td>general</td>\n",
       "      <td>0.996644</td>\n",
       "      <td>no_signal</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>account_closed</td>\n",
       "      <td>0.999721</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no_humiliation</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>hindi</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inform</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>general_ask_inform</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>bank_account</td>\n",
       "      <td>0.953450</td>\n",
       "      <td>general</td>\n",
       "      <td>0.999871</td>\n",
       "      <td>no_signal</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>account_closed</td>\n",
       "      <td>0.975473</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no_humiliation</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>hindi</td>\n",
       "      <td>0.999802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inform</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>general_ask_inform</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>bank_account</td>\n",
       "      <td>0.980187</td>\n",
       "      <td>issue</td>\n",
       "      <td>0.992893</td>\n",
       "      <td>no_signal</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>account_closed</td>\n",
       "      <td>0.763020</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no_humiliation</td>\n",
       "      <td>0.999908</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999953</td>\n",
       "      <td>hindi</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inform</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>general_ask_inform</td>\n",
       "      <td>0.999865</td>\n",
       "      <td>bank_account</td>\n",
       "      <td>0.997983</td>\n",
       "      <td>general</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>no_signal</td>\n",
       "      <td>0.999984</td>\n",
       "      <td>account_general_issue</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no_humiliation</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>hindi</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  intent_pred  intent_acc     sub_intent_pred  sub_intent_acc  context_pred  \\\n",
       "0      inform    0.999714  general_ask_inform        0.999998  bank_account   \n",
       "1      inform    0.999995  general_ask_inform        1.000000  bank_account   \n",
       "2      inform    0.999995  general_ask_inform        1.000000  bank_account   \n",
       "3      inform    0.999991  general_ask_inform        1.000000  bank_account   \n",
       "4      inform    1.000000  general_ask_inform        0.999865  bank_account   \n",
       "\n",
       "   context_acc sub_context_pred  sub_context_acc signal_pred  signal_acc  \\\n",
       "0     0.903529          general         0.999810   no_signal    0.999999   \n",
       "1     0.607388          general         0.996644   no_signal    0.999992   \n",
       "2     0.953450          general         0.999871   no_signal    0.999999   \n",
       "3     0.980187            issue         0.992893   no_signal    0.999915   \n",
       "4     0.997983          general         0.999957   no_signal    0.999984   \n",
       "\n",
       "             reason_pred  reason_acc third_person_pred  third_person_acc  \\\n",
       "0         account_closed    0.958353                no               1.0   \n",
       "1         account_closed    0.999721                no               1.0   \n",
       "2         account_closed    0.975473                no               1.0   \n",
       "3         account_closed    0.763020                no               1.0   \n",
       "4  account_general_issue    0.979352                no               1.0   \n",
       "\n",
       "   humiliate_pred  humiliate_acc sentiment_pred  sentiment_acc language_pred  \\\n",
       "0  no_humiliation       1.000000        neutral       1.000000         hindi   \n",
       "1  no_humiliation       1.000000        neutral       1.000000         hindi   \n",
       "2  no_humiliation       1.000000        neutral       1.000000         hindi   \n",
       "3  no_humiliation       0.999908        neutral       0.999953         hindi   \n",
       "4  no_humiliation       1.000000        neutral       0.999996         hindi   \n",
       "\n",
       "   language_acc  \n",
       "0      0.999990  \n",
       "1      1.000000  \n",
       "2      0.999802  \n",
       "3      0.999998  \n",
       "4      1.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'intent_pred': intent_pred_list,\n",
    "                   'intent_acc': intent_acc_list,\n",
    "                   'sub_intent_pred': sub_intent_pred_list,\n",
    "                   'sub_intent_acc': sub_intent_acc_list,\n",
    "                   'context_pred': context_pred_list,\n",
    "                   'context_acc': context_acc_list,\n",
    "                   'sub_context_pred': sub_context_pred_list,\n",
    "                   'sub_context_acc': sub_context_acc_list,\n",
    "                   'signal_pred': signal_pred_list,\n",
    "                   'signal_acc': signal_acc_list,\n",
    "                   'reason_pred': reason_pred_list,\n",
    "                   'reason_acc': reason_acc_list,\n",
    "                   'third_person_pred': third_person_pred_list,\n",
    "                   'third_person_acc': third_person_acc_list,\n",
    "                   'humiliate_pred': humiliate_pred_list,\n",
    "                   'humiliate_acc': humiliate_acc_list,\n",
    "                   'sentiment_pred': sentiment_pred_list,\n",
    "                   'sentiment_acc': sentiment_acc_list,\n",
    "                   'language_pred': language_pred_list,\n",
    "                   'language_acc': language_acc_list})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ec59400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intent_pred_comp', 'sub_intent_pred_comp', 'context_pred_comp', 'sub_context_pred_comp', 'signal_pred_comp', 'reason_pred_comp', 'third_person_pred_comp', 'humiliate_pred_comp', 'sentiment_pred_comp', 'language_pred_comp']\n"
     ]
    }
   ],
   "source": [
    "J = {'intent_pred_comp' : [], \n",
    "   'sub_intent_pred_comp': [],\n",
    "   'context_pred_comp': [],\n",
    "   'sub_context_pred_comp': [],\n",
    "   'signal_pred_comp': [],\n",
    "   'reason_pred_comp': [],\n",
    "   'third_person_pred_comp': [],\n",
    "   'humiliate_pred_comp': [],\n",
    "   'sentiment_pred_comp': [],\n",
    "   'language_pred_comp': []}\n",
    "\n",
    "K = {'intent_acc_comp' : [], \n",
    "   'sub_intent_acc_comp': [],\n",
    "   'context_acc_comp': [],\n",
    "   'sub_context_acc_comp': [],\n",
    "   'signal_acc_comp': [],\n",
    "   'reason_acc_comp': [],\n",
    "   'third_person_acc_comp': [],\n",
    "   'humiliate_acc_comp': [],\n",
    "   'sentiment_acc_comp': [],\n",
    "   'language_acc_comp': []}\n",
    "\n",
    "L = [\"intent\", \"sub_intent\", \"context\", \"sub_context\", \"signal\", \"delay_reason\", \"third_person\", \"humiliate\", \"sentiment\", \"lang\"]\n",
    "\n",
    "\n",
    "pred_comp_list = list(J.keys())\n",
    "print(pred_comp_list)\n",
    "\n",
    "for i in L:\n",
    "    for index, row in df.iterrows():\n",
    "        x = i\n",
    "        y = i+str(\"_pred\")\n",
    "        \n",
    "\n",
    "        if row[x] == row[y]:\n",
    "            list(i+\"_pred_comp\").append(\"correct_prediction\")\n",
    "        else:\n",
    "            list(i+\"_pred_comp\").append(\"wrong_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b1d54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(intent_pred_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3f0c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
